import cv2
import numpy as np
import streamlit as st
import os
import gdown
from time import time
import io
from datetime import timedelta

# T·∫£i YOLO weights v√† config n·∫øu ch∆∞a c√≥
weights_file = "yolov3.weights"
config_file = "yolov3.cfg"
classes_file = "yolov3.txt"

if not os.path.exists(weights_file):
    gdown.download("https://drive.google.com/uc?id=11rE4um7BB12mtsgiq-D774qprMaRhjpm", weights_file, quiet=False)

if not os.path.exists(config_file) or not os.path.exists(classes_file):
    st.error("Missing YOLO config or classes file.")
    st.stop()

# ƒê·ªçc danh s√°ch c√°c l·ªõp
with open(classes_file, 'r') as f:
    classes = [line.strip() for line in f.readlines()]
COLORS = np.random.uniform(0, 255, size=(len(classes), 3))

# T·∫£i m√¥ h√¨nh YOLO
net = cv2.dnn.readNet(weights_file, config_file)

# H√†m l·∫•y layer ƒë·∫ßu ra
def get_output_layers(net):
    layer_names = net.getLayerNames()
    return [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# Giao di·ªán Streamlit
st.title("Object Detection with YOLO")

# Thanh b√™n tr√°i ƒë·ªÉ nh·∫≠p th√¥ng tin
st.sidebar.header("Settings")
object_names_input = st.sidebar.text_input("Enter Object Names (comma separated)", "cell phone,laptop,umbrella")
object_names = [obj.strip().lower() for obj in object_names_input.split(',')]
monitor_counts = {}
lost_objects_time = {}  # Th√™m t·ª´ ƒëi·ªÉn ƒë·ªÉ theo d√µi th·ªùi gian m·∫•t c·ªßa t·ª´ng ƒë·ªëi t∆∞·ª£ng
for obj in object_names:
    monitor_counts[obj] = st.sidebar.number_input(f"Enter number of {obj} to monitor", min_value=0, value=0, step=1)

frame_limit = st.sidebar.slider("Set Frame Limit for Alarm (seconds)", 1, 10, 3)

# Ch·ªçn ngu·ªìn video
video_source = st.radio("Choose Video Source", ["Upload File"])
temp_video_path = "temp_video.mp4"

# N√∫t ƒëi·ªÅu khi·ªÉn
start_button = st.button("Start Detection")
stop_button = st.button("Stop and Delete Video")

cap = None  # Bi·∫øn ƒë·ªÉ l∆∞u ngu·ªìn video

# ƒê·ªçc file √¢m thanh c·∫£nh b√°o (police.wav)
def play_alert_sound():
    alert_audio_file = '/mnt/data/police.wav'  # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file √¢m thanh police.wav
    if os.path.exists(alert_audio_file):
        with open(alert_audio_file, 'rb') as f:
            audio_bytes = f.read()
            st.audio(audio_bytes, format='audio/wav')

# X·ª≠ l√Ω video t·ª´ ngu·ªìn
if video_source == "Upload File":
    uploaded_file = st.file_uploader("Upload a video file", type=["mp4", "avi", "mov", "mkv"])
    if uploaded_file is not None:
        with open(temp_video_path, "wb") as f:
            f.write(uploaded_file.read())
        st.success("Video uploaded successfully!")
        cap = cv2.VideoCapture(temp_video_path)

# Ki·ªÉm tra n·∫øu c√≥ video ƒë·ªÉ x·ª≠ l√Ω
if cap is not None and start_button:
    stframe = st.empty()
    detected_objects = {}
    lost_objects_time = {}
    alerted_objects = set()  # ƒê·ªÉ theo d√µi c√°c ƒë·ªëi t∆∞·ª£ng ƒë√£ c·∫£nh b√°o
    previously_detected = set()  # ƒê·ªÉ theo d√µi c√°c ƒë·ªëi t∆∞·ª£ng ƒë√£ ƒë∆∞·ª£c ph√°t hi·ªán trong c√°c khung h√¨nh tr∆∞·ªõc
    start_time = time()

    while True:
        ret, frame = cap.read()
        if not ret:
            st.warning("Video ended or no frames available.")
            break

        # Ph√°t hi·ªán v·∫≠t th·ªÉ
        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
        net.setInput(blob)
        outs = net.forward(get_output_layers(net))

        height, width, _ = frame.shape
        boxes = []
        class_ids = []
        confidences = []
        detected_objects.clear()

        # L·∫•y th√¥ng tin t·ª´ c√°c l·ªõp ƒë·∫ßu ra
        for out in outs:
            for detection in out:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                if confidence > 0.5:
                    center_x = int(detection[0] * width)
                    center_y = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    x = center_x - w // 2
                    y = center_y - h // 2
                    boxes.append([x, y, w, h])
                    class_ids.append(class_id)
                    confidences.append(float(confidence))

        # √Åp d·ª•ng Non-Maximum Suppression ƒë·ªÉ lo·∫°i b·ªè c√°c bounding box ch·ªìng l·∫•p
        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

        if len(indices) > 0:
            for i in indices.flatten():
                x, y, w, h = boxes[i]
                label = classes[class_ids[i]].lower()
                color = COLORS[class_ids[i]]

                # V·∫Ω bounding box v√† nh√£n
                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

                # ƒê·∫øm v√† theo d√µi
                if label in detected_objects:
                    detected_objects[label] += 1
                else:
                    detected_objects[label] = 1

                # N·∫øu v·∫≠t th·ªÉ ch∆∞a ƒë∆∞·ª£c b√°o quay l·∫°i, th√¥ng b√°o
                if label not in previously_detected:
                    previously_detected.add(label)
                    if label in lost_objects_time:
                        # ƒê√°nh d·∫•u v·∫≠t th·ªÉ quay l·∫°i
                        lost_duration = time() - lost_objects_time[label]
                        lost_time_str = str(timedelta(seconds=int(lost_duration)))
                        st.success(f"üîî '{label}' is back after {lost_time_str}!")
                        lost_objects_time.pop(label)  # X√≥a kh·ªèi danh s√°ch m·∫•t

        # Ki·ªÉm tra v·∫≠t th·ªÉ thi·∫øu
        current_time = time()
        for obj in object_names:
            required_count = monitor_counts.get(obj, 0)
            current_count = detected_objects.get(obj, 0)

            if current_count < required_count:
                if obj not in lost_objects_time:
                    lost_objects_time[obj] = current_time
            else:
                if obj in lost_objects_time:
                    del lost_objects_time[obj]
                if obj in alerted_objects:
                    alerted_objects.remove(obj)

        # Hi·ªÉn th·ªã video
        stframe.image(frame, channels="BGR", use_container_width=True)

if stop_button:
    if cap:
        cap.release()
    if os.path.exists(temp_video_path):
        os.remove(temp_video_path)
    st.success("Video stopped and temporary file deleted.")
